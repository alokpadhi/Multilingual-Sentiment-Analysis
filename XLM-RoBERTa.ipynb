{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4981f385-840f-4145-97a9-d3e686fc41ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 26 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (59.5.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-62.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (0.35.1)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.35.1\n",
      "    Uninstalling wheel-0.35.1:\n",
      "      Successfully uninstalled wheel-0.35.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 59.5.0\n",
      "    Uninstalling setuptools-59.5.0:\n",
      "      Successfully uninstalled setuptools-59.5.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient-utils 0.5.0 requires wheel<0.36.0,>=0.35.1, but you have wheel 0.37.1 which is incompatible.\u001b[0m\n",
      "Successfully installed pip-22.1.2 setuptools-62.6.0 wheel-0.37.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da18e3f2-0097-4dd5-9517-5cb053f0b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting indic_transliteration\n",
      "  Downloading indic_transliteration-2.3.31-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typer in /opt/conda/lib/python3.8/site-packages (from indic_transliteration) (0.4.0)\n",
      "Collecting roman\n",
      "  Downloading roman-3.3-py2.py3-none-any.whl (3.9 kB)\n",
      "Collecting aksharamukha\n",
      "  Downloading aksharamukha-2.0.3-py3-none-any.whl (231 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m261.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: backports.functools-lru-cache in /opt/conda/lib/python3.8/site-packages (from indic_transliteration) (1.6.4)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.8/site-packages (from indic_transliteration) (0.10.2)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.4/981.4 kB\u001b[0m \u001b[31m80.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from indic_transliteration) (2022.1.18)\n",
      "Requirement already satisfied: Requests>=2.20.1 in /opt/conda/lib/python3.8/site-packages (from aksharamukha->indic_transliteration) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.8/site-packages (from aksharamukha->indic_transliteration) (5.4.1)\n",
      "Requirement already satisfied: langcodes>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from aksharamukha->indic_transliteration) (3.3.0)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m110.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:06\u001b[0mm\n",
      "\u001b[?25hCollecting pykakasi>=2.0.6\n",
      "  Downloading pykakasi-2.2.1-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting language-data\n",
      "  Downloading language_data-1.1-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fonttools[unicode]>=4.31\n",
      "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.9/930.9 kB\u001b[0m \u001b[31m150.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trio~=0.17\n",
      "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.0/359.0 kB\u001b[0m \u001b[31m194.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3[secure,socks]~=1.26 in /opt/conda/lib/python3.8/site-packages (from selenium->indic_transliteration) (1.26.7)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.8/site-packages (from typer->indic_transliteration) (8.0.3)\n",
      "Collecting unicodedata2>=14.0.0\n",
      "  Downloading unicodedata2-14.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (467 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m173.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting jaconv\n",
      "  Downloading jaconv-0.3.tar.gz (15 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from Requests>=2.20.1->aksharamukha->indic_transliteration) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from Requests>=2.20.1->aksharamukha->indic_transliteration) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from Requests>=2.20.1->aksharamukha->indic_transliteration) (2.0.9)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m136.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.8/site-packages (from trio~=0.17->selenium->indic_transliteration) (1.2.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.8/site-packages (from trio~=0.17->selenium->indic_transliteration) (2.4.0)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/conda/lib/python3.8/site-packages (from urllib3[secure,socks]~=1.26->selenium->indic_transliteration) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in /opt/conda/lib/python3.8/site-packages (from urllib3[secure,socks]~=1.26->selenium->indic_transliteration) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in /opt/conda/lib/python3.8/site-packages (from urllib3[secure,socks]~=1.26->selenium->indic_transliteration) (36.0.1)\n",
      "Collecting marisa-trie<0.8.0,>=0.7.7\n",
      "  Downloading marisa_trie-0.7.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m150.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.8/site-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium->indic_transliteration) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from marisa-trie<0.8.0,>=0.7.7->language-data->aksharamukha->indic_transliteration) (62.6.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.8/site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium->indic_transliteration) (1.16.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m147.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m164.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium->indic_transliteration) (2.21)\n",
      "Building wheels for collected packages: jaconv\n",
      "  Building wheel for jaconv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jaconv: filename=jaconv-0.3-py3-none-any.whl size=15566 sha256=b62953198b58b7ad3521a4352d51974ea470414b25101167ddb9dd288fa9220e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7uzrjr2y/wheels/73/e8/fb/b4ad8117719f79ac73bc05406d1768f845688cdbeed7aad87e\n",
      "Successfully built jaconv\n",
      "Installing collected packages: unicodedata2, jaconv, wrapt, roman, marisa-trie, lxml, h11, fonttools, attrs, async-generator, wsproto, outcome, language-data, deprecated, trio, pykakasi, trio-websocket, aksharamukha, selenium, indic_transliteration\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.29.1\n",
      "    Uninstalling fonttools-4.29.1:\n",
      "      Successfully uninstalled fonttools-4.29.1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 18.2.0\n",
      "    Uninstalling attrs-18.2.0:\n",
      "      Successfully uninstalled attrs-18.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 1.10.0 requires attrs<=19, but you have attrs 21.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aksharamukha-2.0.3 async-generator-1.10 attrs-21.4.0 deprecated-1.2.13 fonttools-4.33.3 h11-0.13.0 indic_transliteration-2.3.31 jaconv-0.3 language-data-1.1 lxml-4.9.0 marisa-trie-0.7.7 outcome-1.2.0 pykakasi-2.2.1 roman-3.3 selenium-4.3.0 trio-0.21.0 trio-websocket-0.9.2 unicodedata2-14.0.0 wrapt-1.14.1 wsproto-1.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: contractions in /opt/conda/lib/python3.8/site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.8/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /opt/conda/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m100.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:02\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m187.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m114.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.19-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m93.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m57.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.26.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.9.0)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from wandb) (62.6.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (8.0.3)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.6.0-py2.py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m219.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from wandb) (5.4.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m103.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=e52518af12e0bf9b5161f39228b8a2012ef1d8d1782ccae3209f15ed2a760374\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-63_752xu/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8792 sha256=9e9bf2fb35b44ae8d3e605177c31610be24ee8b702ca7ff07c60cceb7a0701cc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-63_752xu/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: pathtools, smmap, shortuuid, setproctitle, sentry-sdk, promise, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 sentry-sdk-1.6.0 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.19\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install indic_transliteration\n",
    "!pip install contractions\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06340b6-6dc1-4799-8c1b-89232f9fbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import string\n",
    "import contractions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd166ec1-8225-4bf7-87bb-c48343144b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c458ab09-dea8-4565-90ca-b9874f64a361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malokpadhi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4610397-0996-45b5-ab50-2443f5f0c64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0848e745-192b-4e48-b6c6-7d41c9c0a6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'येह् तोह् अद्भुत हे'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate(\"yeh toh अद्भुत he\", sanscript.ITRANS, sanscript.DEVANAGARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47d446d-b373-4d4c-8b8b-3300c5a2de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1000758-e03e-4963-9d9b-ddd5f8de4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artifacts/run-3rxv0ijv-ProcessTrainDataframe:v0/Process Train Dataframe.table.json\", 'r') as fp:\n",
    "    train_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d97ceb1-598e-497f-86a1-9bc9c8516fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nen á vist bolest vztek smutek zmatek osam ě l...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haan yaar neha 😔😔 kab karega woh post 😭 Usne n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>television media congress ke liye nhi h . Ye t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All India me nrc lagu kare w Kashmir se dhara ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pagal hai kya ? They aren ’ t real issues Mand...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment  \\\n",
       "0  nen á vist bolest vztek smutek zmatek osam ě l...   neutral   \n",
       "1  Haan yaar neha 😔😔 kab karega woh post 😭 Usne n...   neutral   \n",
       "2  television media congress ke liye nhi h . Ye t...  negative   \n",
       "3  All India me nrc lagu kare w Kashmir se dhara ...  positive   \n",
       "4  Pagal hai kya ? They aren ’ t real issues Mand...   neutral   \n",
       "\n",
       "   Sentiment Label  \n",
       "0                2  \n",
       "1                2  \n",
       "2                1  \n",
       "3                0  \n",
       "4                2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_data[\"data\"], columns=train_data[\"columns\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54949564-1a85-4e92-9d9e-c5f69542dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artifacts/run-3rxv0ijv-ProcessTestDataframe:v0/Process Test Dataframe.table.json\", 'r') as fp:\n",
    "    test_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e88b41-a2af-443a-ba3c-49bafa9cff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modi mantrimandal may samil honay par badhai n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tu toh naamakool hai Mare h</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOU saw caste and religion in them ... nation ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sir local police station pe complaint krne par...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ve Maahi song from # Kesari is current favouri...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment  \\\n",
       "0  modi mantrimandal may samil honay par badhai n...  positive   \n",
       "1                        Tu toh naamakool hai Mare h  negative   \n",
       "2  YOU saw caste and religion in them ... nation ...  negative   \n",
       "3  sir local police station pe complaint krne par...   neutral   \n",
       "4  Ve Maahi song from # Kesari is current favouri...  positive   \n",
       "\n",
       "   Sentiment Label  \n",
       "0                0  \n",
       "1                1  \n",
       "2                1  \n",
       "3                2  \n",
       "4                0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(test_data[\"data\"], columns=test_data[\"columns\"])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd6a7488-03be-450a-a216-60cdb4d9e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b18a172-e143-4b94-a0de-2336b5ee4324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Data Exploration and MLP.ipynb'   README.md           \u001b[0m\u001b[01;34mdata\u001b[0m/     \u001b[01;34mwandb\u001b[0m/\n",
      " LICENSE                           XLM-RoBERTa.ipynb   \u001b[01;34mimages\u001b[0m/\n",
      " LSTM.ipynb                        \u001b[01;34martifacts\u001b[0m/          \u001b[01;34mmodel\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31ae8bfc-ca93-4dcf-abc8-f983ecb8f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = Path(\"data/train_14k_split_conll.txt\")\n",
    "test_file = Path(\"data/dev_3k_split_conll.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "713fdaeb-2555-4c98-a0c5-6cee17e428b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files\n",
    "with open(train_file) as f:\n",
    "    train_data = f.readlines()\n",
    "    \n",
    "with open(test_file, 'r') as f:\n",
    "    test_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "465c0f3d-4eb0-4ceb-8b73-47bbd797704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data\n",
    "def parse_data(data):\n",
    "    uids, sentences, sentences_info, sentiment = [], [], [], []\n",
    "    all_langs = []\n",
    "    \n",
    "    single_sentence, single_sentence_info = [], []\n",
    "    sent = \"\"\n",
    "    uid = 0\n",
    "    \n",
    "    for i, line in enumerate(data):\n",
    "        line_ = line.strip()\n",
    "        tokens = line_.split('\\t')\n",
    "        num_tokens = len(tokens)\n",
    "        if num_tokens == 2:\n",
    "            # add the word\n",
    "            single_sentence.append(tokens[0])\n",
    "            # add the language\n",
    "            single_sentence_info.append(tokens[1])\n",
    "            # all_langs.append(tokens[1])\n",
    "        elif num_tokens == 3 and i > 0:\n",
    "            sentences.append(single_sentence)\n",
    "            sentences_info.append(single_sentence_info)\n",
    "            sentiment.append(sent)\n",
    "            uids.append(uid)\n",
    "            sent = tokens[-1]\n",
    "            uid = int(tokens[1])\n",
    "            single_sentence = []\n",
    "            single_sentence_info = []\n",
    "        elif num_tokens == 1:\n",
    "            continue\n",
    "        else:\n",
    "            sent = tokens[-1]\n",
    "            uid = int(tokens[1])\n",
    "            \n",
    "    if len(single_sentence) > 0:\n",
    "        sentences.append(single_sentence)\n",
    "        sentences_info.append(single_sentence_info)\n",
    "        sentiment.append(sent)\n",
    "        uids.append(uid)\n",
    "        \n",
    "    assert len(sentences) == len(sentences_info) == len(sentiment) == len(uids)\n",
    "    return sentences, sentences_info, sentiment, uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85c5dda5-57eb-4839-ac39-dcd5c160dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, sentences_info, sentiment, uids = parse_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56cf85e6-e029-4ff4-b223-34df42cc9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences, test_sentences_info, test_sentiment, test_uids = parse_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e93faa66-9fc7-4542-b86d-21a7cc43dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentences, sentences_info):\n",
    "    translated_sentneces = []\n",
    "    for sent, sent_info in zip(sentences, sentences_info):\n",
    "        translated_sentence = []\n",
    "        for word, word_info in zip(sent, sent_info):\n",
    "            if word_info == \"Hin\":\n",
    "                translated_sentence.append(transliterate(word, sanscript.ITRANS, sanscript.DEVANAGARI))\n",
    "            else:\n",
    "                translated_sentence.append(word)\n",
    "        translated_sentneces.append(translated_sentence)\n",
    "    return translated_sentneces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b24e8b65-b0c8-452b-afb9-d8dfa1b72037",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentences = translate(sentences, sentences_info)\n",
    "test_translated_sentences = translate(test_sentences, test_sentences_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "761bab03-48ba-481c-ace1-a32ef9de98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "url_pattern = r'https(.*)/\\s\\w+'\n",
    "names_with_numbers = r'([A-Za-z]+)\\d{3,}'\n",
    "apostee = r\"([\\w]+)\\s'\\s([\\w]+)\"\n",
    "names = r\"@[\\s]*[\\w]+[\\s]*[_]+[\\s]*[\\w]+|@[\\s]*[\\w]+\"\n",
    "\n",
    "def preprocess_data(sentence_tokens):\n",
    "    sentence = \" \".join(sentence_tokens)\n",
    "    sentence = \" \" + sentence\n",
    "    \n",
    "    # Remove RT and ...\n",
    "    sentence = re.sub(\"\\sRT\\s\", \"\", sentence)\n",
    "    sentence = sentence.replace(\"…\", \"\")\n",
    "    sentence = re.sub(re.compile(names), \" \", sentence)\n",
    "    sentence = re.sub(re.compile(url_pattern), \"\", sentence)\n",
    "    sentence = re.sub(re.compile(apostee), r\"\\1'\\2\", sentence)\n",
    "    \n",
    "    # fixing contractions\n",
    "    sentence = contractions.fix(sentence)\n",
    "    \n",
    "    sentence = re.sub(re.compile(names_with_numbers), r\" \", sentence)\n",
    "    \n",
    "    sentence = \" \".join(sentence.split()).strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6071352b-fe49-475f-8090-20f255044352",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9991ba59-92ab-4d4f-a947-097aa0422fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "xlmr_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e160a61-c81c-41c8-b0d9-fc6572d156c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all training samples\n",
    "processed_sentences = []\n",
    "for sent in sentences:\n",
    "    processed_sentences.append(preprocess_data(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff30c88c-64d9-4ddb-8e7c-1bf863baf4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed_sentences = []\n",
    "for sent in test_sentences:\n",
    "    test_processed_sentences.append(preprocess_data(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6802b9ef-1aab-4b44-9586-856eb17df4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {\n",
    "    \"positive\": 0,\n",
    "    \"negative\": 1,\n",
    "    \"neutral\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1de204c-2cd2-4741-9503-d3ab62ce356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [sentiment_mapping[sent] for sent in sentiment]\n",
    "test_labels = [sentiment_mapping[sent] for sent in test_sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3873a9a-3b4c-4d83-b4a5-0726e92d62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uids, val_uids, train_X, val_X, train_y, val_y = train_test_split(uids, processed_sentences, labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c3adf0d-7327-4409-9583-1f90e4137329",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_lengths = [len(sent.split()) for sent in train_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2dba4e1-78b5-4608-ba68-60276aa6d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max(train_token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4cc8fab-9971-48fb-a2ef-86363ddfbff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HinglishDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, tokenizer, max_len):\n",
    "        self.sentences = inputs\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        sentiment = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"text\": sentence,\n",
    "            \"input_ids\": encoding['input_ids'].flatten(),\n",
    "            \"attention_mask\": encoding['attention_mask'].flatten(),\n",
    "            \"label\": torch.tensor(sentiment, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def create_dataloader(self, batch_size, shuffle=False):\n",
    "        return DataLoader(dataset=self, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c54690f2-b23a-4895-bc57-fe42b1c5f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HinglishDataset(train_X, train_y, tokenizer, MAX_LEN)\n",
    "val_dataset = HinglishDataset(val_X, val_y, tokenizer, MAX_LEN)\n",
    "test_dataset = HinglishDataset(test_processed_sentences, test_labels, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99f419d0-29c9-4dc2-8a0f-460f788a5f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d558fca0-1812-4dd1-85e5-496f6c27ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = train_dataset.create_dataloader(batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f9b7aa3-8e11-4f57-ab47-fd26817bad4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc80b622-4f97-403a-9039-a5b4491dd6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaConfig {\n",
       "  \"_name_or_path\": \"xlm-roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"XLMRobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"xlm-roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250002\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f09050c-a80a-4c07-b5a6-3c0d46650a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMModel(nn.Module):\n",
    "    def __init__(self, output_dim, xlmr_model, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.xlmr = xlmr_model\n",
    "        hidden_size = self.xlmr.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        xlmr_output = self.xlmr(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # print(pooled_output)\n",
    "        logits = self.out(self.dropout(xlmr_output.pooler_output))\n",
    "        \n",
    "        return logits        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88ba58b9-7c3f-427d-a9a8-54ca950014b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 3\n",
    "xlmr_model = xlmr_model.to(device)\n",
    "model = XLMModel(output_dim, xlmr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f323c4e1-cdfe-4ba4-8dd2-664356530cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5085c0f4-eb08-461a-acaf-fc36d765c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 3e-5\n",
    "min_lr = 5e-6\n",
    "lr_decay = 0.5\n",
    "lr_patience = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbd5031e-1aca-452a-a28d-9af399be3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', lr_decay, lr_patience, verbose=True, min_lr=min_lr)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15fbb664-072c-4f14-b920-b131e4165b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, clip=2.0):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        targets = batch[\"label\"].to(device)\n",
    "        \n",
    "        predictions = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eca2aa9f-62e6-4705-b885-244c414b07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return np.equal(preds, labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2b0565e-93c2-4187-848d-ddc76a672187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            predictions = model(\n",
    "                input_ids,\n",
    "                attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = loss_fn(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            targets.extend(labels.detach().cpu().numpy().tolist())\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            preds.extend(predicted.detach().cpu().numpy().tolist())\n",
    "            \n",
    "        return epoch_loss/len(iterator), simple_accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1977525a-25d8-4e70-9f0f-8a71ec143b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea329960-060b-4829-8594-d9aabb6977b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    learning_rate=lr,\n",
    "    scheduler=scheduler,\n",
    "    weight_decay=lr_decay,\n",
    "    patience=lr_patience,\n",
    "    device=device,\n",
    "    model_name=\"XLMRoBERTaBase\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af0806e9-70bd-40dd-92d4-3b4b3725e0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/Multilingual-Sentiment-Analysis/wandb/run-20220624_190234-19gqdgnb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alokpadhi/Sentiment%20Analysis-Hinglish/runs/19gqdgnb\" target=\"_blank\">XLM_RoBERTa</a></strong> to <a href=\"https://wandb.ai/alokpadhi/Sentiment%20Analysis-Hinglish\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlm_run = wandb.init(project=\"Sentiment Analysis-Hinglish\", config=config, name=\"XLM_RoBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4889bc8-6898-4082-9698-73a679f38605",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model/xlm_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c561164a-160c-40bf-86de-16e1087ef6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting of Training for Epoch: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished Training for Epoch: 1.00\n",
      "[INFO] Starting of Validating for Epoch: 1.00\n",
      "[INFO] Validation for Epoch: 1.00 finished.\n",
      "Epoch: 01 | Time Taken: 1mins:28.00secs\n",
      "Train loss: 0.964 | val_loss: 0.820 | val_accuracy:0.63\n",
      "[INFO] Starting of Training for Epoch: 2.00\n",
      "[INFO] Finished Training for Epoch: 2.00\n",
      "[INFO] Starting of Validating for Epoch: 2.00\n",
      "[INFO] Validation for Epoch: 2.00 finished.\n",
      "Epoch: 02 | Time Taken: 1mins:29.00secs\n",
      "Train loss: 0.834 | val_loss: 0.895 | val_accuracy:0.59\n",
      "[INFO] Starting of Training for Epoch: 3.00\n",
      "[INFO] Finished Training for Epoch: 3.00\n",
      "[INFO] Starting of Validating for Epoch: 3.00\n",
      "[INFO] Validation for Epoch: 3.00 finished.\n",
      "Epoch: 03 | Time Taken: 1mins:27.00secs\n",
      "Train loss: 0.758 | val_loss: 0.808 | val_accuracy:0.64\n",
      "[INFO] Starting of Training for Epoch: 4.00\n",
      "[INFO] Finished Training for Epoch: 4.00\n",
      "[INFO] Starting of Validating for Epoch: 4.00\n",
      "[INFO] Validation for Epoch: 4.00 finished.\n",
      "Epoch: 04 | Time Taken: 1mins:29.00secs\n",
      "Train loss: 0.685 | val_loss: 0.844 | val_accuracy:0.64\n",
      "[INFO] Starting of Training for Epoch: 5.00\n",
      "[INFO] Finished Training for Epoch: 5.00\n",
      "[INFO] Starting of Validating for Epoch: 5.00\n",
      "[INFO] Validation for Epoch: 5.00 finished.\n",
      "Epoch: 05 | Time Taken: 1mins:29.00secs\n",
      "Train loss: 0.600 | val_loss: 1.008 | val_accuracy:0.62\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "wandb.watch(model, log_freq=100)\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(f\"[INFO] Starting of Training for Epoch: {epoch+ 1:.2f}\")\n",
    "    train_loss = train(model, train_dataloader)\n",
    "    print(f\"[INFO] Finished Training for Epoch: {epoch+1:.2f}\")\n",
    "    print(f\"[INFO] Starting of Validating for Epoch: {epoch+1:.2f}\")\n",
    "    val_loss, val_acc = evaluate(model, val_dataloader)\n",
    "    print(f\"[INFO] Validation for Epoch: {epoch+1:.2f} finished.\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    \n",
    "    wandb.log({\"train_loss\": train_loss})\n",
    "    wandb.log({\"val_loss\": val_loss})\n",
    "    wandb.log({\"val_accuracy\": val_acc})\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f\"Epoch: {epoch + 1:02} | Time Taken: {epoch_mins}mins:{epoch_secs:.2f}secs\")\n",
    "    print(f\"Train loss: {train_loss:.3f} | val_loss: {val_loss:.3f} | val_accuracy:{val_acc:.2f}\")\n",
    "    \n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        xlm_run.summary[\"Best validation Loss\"] = best_valid_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        model_artifact = wandb.Artifact(\"XLMRoBERTaModel\", type=\"model\")\n",
    "        model_artifact.add_file(model_path)\n",
    "        xlm_run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "71e64709-7292-4198-b5fa-76702ea91cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"this is\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6be16abe-e9fd-4195-a623-b685dfc8b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "754b730a-31b1-48c9-914e-2756de0a6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = xlmr_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74c62869-dfc4-4a40-a752-32ebd4389e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2938198a-4017-446a-b44d-84c4c12b2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trgs = []\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        targets = batch[\"label\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids, attention_mask\n",
    "        )\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds.extend(predicted.detach().cpu().numpy().tolist())\n",
    "        trgs.extend(targets.detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c8d3e4a-2691-4106-897e-2c05ed72bee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69       982\n",
      "           1       0.62      0.72      0.66       890\n",
      "           2       0.61      0.41      0.49      1128\n",
      "\n",
      "    accuracy                           0.62      3000\n",
      "   macro avg       0.62      0.63      0.61      3000\n",
      "weighted avg       0.62      0.62      0.61      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(trgs, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71bf1972-4ef3-43fb-bcf8-3bf7387774db",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = metrics.precision_recall_fscore_support(trgs, preds, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aeac3899-8ddf-4aa8-a6c3-6688e8117b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"precision\": performance[0], \"recall\": performance[1], \"f1-score\": performance[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e06f0-1eed-45b3-a464-d47b9066a7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
