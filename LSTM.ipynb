{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19afe009-b7b6-48f4-8cc6-7e7e6c687a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292 kB 19.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.22.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.29.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e62df7c-f461-42c7-9b60-7be3efe715c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (110 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110 kB 29.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 287 kB 29.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3106a4-77a1-4d8b-ab65-43eb9ac94d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.18-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8 MB 19.2 MB/s eta 0:00:01     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 1.0 MB 19.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from wandb) (59.5.0)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (8.0.3)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from wandb) (5.4.1)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145 kB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 22.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.1)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=9266ed6a1ff8a0972b481f04a5f043e7a5fa0eeb44294d5bf85dd548d8155639\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bz5_iisp/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=c73a31428b871da5b689bed9b7049cafdf08c432dd52bd64305a53a2fd375add\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bz5_iisp/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, promise, pathtools, GitPython, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.18\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4540a346-85c5-4421-b6ff-8839c9b8b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c62f1d3e-a647-450d-9645-0abc986b2e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b085ff-43c5-4fa7-abe3-de0766ef8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malokpadhi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/Multilingual-Sentiment-Analysis/wandb/run-20220621_174359-v2kujjdb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alokpadhi/Multilingual-Sentiment-Analysis/runs/v2kujjdb\" target=\"_blank\">stoic-terrain-1</a></strong> to <a href=\"https://wandb.ai/alokpadhi/Multilingual-Sentiment-Analysis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('alokpadhi/Sentiment Analysis-Hinglish/run-3rxv0ijv-ProcessTrainDataframe:v0', type='run_table')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff44a639-4ca7-4cfe-a7ac-2d05ae31c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = run.use_artifact('alokpadhi/Sentiment Analysis-Hinglish/run-3rxv0ijv-ProcessTestDataframe:v0', type='run_table')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63cb0871-904d-4fce-b68c-323c5bab7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39149053-0750-4624-a071-399064c13e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "035d64a1-68e8-4ffa-83bf-351f31833290",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artifacts/run-3rxv0ijv-ProcessTrainDataframe:v0/Process Train Dataframe.table.json\", 'r') as fp:\n",
    "    train_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8415e0c1-6867-40cd-9fe0-53f44fd925d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nen Ã¡ vist bolest vztek smutek zmatek osam Ä› l...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haan yaar neha ðŸ˜”ðŸ˜” kab karega woh post ðŸ˜­ Usne n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>television media congress ke liye nhi h . Ye t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All India me nrc lagu kare w Kashmir se dhara ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pagal hai kya ? They aren â€™ t real issues Mand...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment  \\\n",
       "0  nen Ã¡ vist bolest vztek smutek zmatek osam Ä› l...   neutral   \n",
       "1  Haan yaar neha ðŸ˜”ðŸ˜” kab karega woh post ðŸ˜­ Usne n...   neutral   \n",
       "2  television media congress ke liye nhi h . Ye t...  negative   \n",
       "3  All India me nrc lagu kare w Kashmir se dhara ...  positive   \n",
       "4  Pagal hai kya ? They aren â€™ t real issues Mand...   neutral   \n",
       "\n",
       "   Sentiment Label  \n",
       "0                2  \n",
       "1                2  \n",
       "2                1  \n",
       "3                0  \n",
       "4                2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_data[\"data\"], columns=train_data[\"columns\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34c4f0ce-c204-49e0-83a2-44c19422636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artifacts/run-3rxv0ijv-ProcessTestDataframe:v0/Process Test Dataframe.table.json\", 'r') as fp:\n",
    "    test_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea4654d3-2027-472f-8293-06e84d49cb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modi mantrimandal may samil honay par badhai n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tu toh naamakool hai Mare h</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOU saw caste and religion in them ... nation ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sir local police station pe complaint krne par...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ve Maahi song from # Kesari is current favouri...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment  \\\n",
       "0  modi mantrimandal may samil honay par badhai n...  positive   \n",
       "1                        Tu toh naamakool hai Mare h  negative   \n",
       "2  YOU saw caste and religion in them ... nation ...  negative   \n",
       "3  sir local police station pe complaint krne par...   neutral   \n",
       "4  Ve Maahi song from # Kesari is current favouri...  positive   \n",
       "\n",
       "   Sentiment Label  \n",
       "0                0  \n",
       "1                1  \n",
       "2                1  \n",
       "3                2  \n",
       "4                0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(test_data[\"data\"], columns=test_data[\"columns\"])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2177d98f-d7fb-48a1-a413-811c9c7dc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import string\n",
    "import contractions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e3fe5fc-27c6-4a3a-b970-40fe21e93c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96de2795-582e-4f3a-ad28-ba3b1e65432c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09f20a1c-9b2b-47a7-86c5-187ba16f05ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf449f71-9ea1-48a1-95c2-e9f56ac42d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.Sentence.to_numpy()\n",
    "y = train_df[\"Sentiment Label\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2bc9541-24f7-4dd1-8c2b-48f09c1b123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.Sentence.to_numpy()\n",
    "y_test = test_df[\"Sentiment Label\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9099c16f-6dac-43cd-aae3-74ca4678453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a551720-2d65-4374-9c7b-6cedf6caf3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11200, 2800, 3000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b850b53-6e2e-4b11-9dff-314f89348977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "            \n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d953974-190e-418d-bc3c-11993bc7e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, threshold=15):\n",
    "    counter = Counter()\n",
    "    for i, sent in enumerate(sentences):\n",
    "        counter.update(sent.split())\n",
    "    \n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "    \n",
    "    vocab = Vocab()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "918eb228-52c0-417e-a537-db2324eb9f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d8bff0f-6438-459a-a2e6-8aeb15e3e553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1607"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ecf152e-36c0-4df6-acab-7615a08d22ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab('<start>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f076afa-a3d4-472e-9e72-2096240d28c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HinglishDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.sentences = inputs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        sentiment = self.labels[idx]\n",
    "        \n",
    "        tokens = []\n",
    "        tokens.append(vocab('<start>'))\n",
    "        for token in sentence.split():\n",
    "            tokens.append(vocab(token))\n",
    "        tokens.append(vocab('<end>'))\n",
    "        return torch.LongTensor(tokens), sentiment\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        data.sort(key=lambda x:len(x[0]), reverse=True)\n",
    "        sentences, sentiments = zip(*data)\n",
    "        \n",
    "        sent_lengths = [len(sent) for sent in sentences]\n",
    "        inputs = torch.zeros((len(sentences), max(sent_lengths)), dtype=torch.long)\n",
    "        labels = torch.zeros(len(sentences), dtype=torch.long)\n",
    "        \n",
    "        for idx, sent in enumerate(sentences):\n",
    "            end = sent_lengths[idx]\n",
    "            inputs[idx, :end] = sent[:end]\n",
    "            labels[idx] = sentiments[idx]\n",
    "            \n",
    "        return inputs, sent_lengths, labels\n",
    "    \n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return DataLoader(dataset=self, batch_size=batch_size, collate_fn=self.collate_fn, \n",
    "                         shuffle=shuffle, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "297b71fd-7ed6-4c27-a245-4692cc934485",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HinglishDataset(X_train, y_train)\n",
    "val_dataset = HinglishDataset(X_val, y_val)\n",
    "test_dataset = HinglishDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d346b51-8b15-40dc-999a-0f328c73e99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  3,  4,  3,  5,  6,  3,  7,  8,  9, 10,  6,  3, 10,  6,  3, 10, 11,\n",
       "          6,  3,  2]),\n",
       " 0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28b8602c-bfde-4c80-b5fd-70143bd0d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = train_dataset.create_dataloader(batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4662caef-a804-409e-a9e5-669956f951d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 32]), 64, torch.Size([64]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample = next(iter(train_dataloader))\n",
    "train_sample[0].shape, len(train_sample[1]), train_sample[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f1ab425a-47ff-46db-b0aa-588c3c6360e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, dropout=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_dim,\n",
    "            hid_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hid_dim, 50)\n",
    "        self.out = nn.Linear(50, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, input_lengths):\n",
    "        # inputs => [batch_size, seq_len]\n",
    "        # input_lengths => [batch_size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)\n",
    "        _, hidden = self.rnn(packed_input)\n",
    "\n",
    "        hidden = hidden.view(self.num_layers, 2, -1, self.hid_dim)\n",
    "        final_forward_hidden = hidden[-1, -2, :, :]\n",
    "        final_backward_hidden = hidden[-1, -1, :, :]\n",
    "        # final_*_hidden => [batch_size, hidden_dim]\n",
    "\n",
    "        combined = final_forward_hidden + final_backward_hidden\n",
    "        combined = self.dropout(combined)\n",
    "        # combined => [batch_size, hidden_dim]\n",
    "\n",
    "        intermediate = F.relu(self.fc(combined))\n",
    "        intermediate = self.dropout(intermediate)\n",
    "        logits = self.out(intermediate)\n",
    "        # logits => [batch_size, output_dim]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02ec0659-6438-4e5c-9a63-1b20a83ea3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "output_dim = 3\n",
    "emb_dim = 200\n",
    "hid_dim = 100\n",
    "num_layers = 2\n",
    "NUM_EPOCHS = 20\n",
    "model_path = \"model/gru_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de1b9b7d-ae6b-492b-a8e8-23acd50ce454",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUModel(input_dim, emb_dim, hid_dim, output_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e5fdb1d-e949-4ddb-b6e8-451e6cbce095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUModel(\n",
       "  (embedding): Embedding(1607, 200)\n",
       "  (rnn): GRU(200, 100, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (fc): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (out): Linear(in_features=50, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f5f9007-9b58-4fc7-b86e-dee0d6744e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "min_lr = 3e-5\n",
    "lr_decay=0.5\n",
    "lr_patience=2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', lr_decay, lr_patience, verbose=True, min_lr=min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f82d1851-d054-4a65-aeb7-ab1931a31858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterator, clip=2.0):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        sentences = batch[0].to(device)\n",
    "        sentence_lengths = batch[1]\n",
    "        targets = batch[2].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(sentences, sentence_lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if i % 2000 == 1999:\n",
    "            wandb.log({\"train_loss\": epoch_loss/2000})\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94fe2a22-0c44-4df2-b02c-f3f449f605a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "96511dec-50ac-4d78-b072-26020de497a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(iterator):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            sentences = batch[0].to(device)\n",
    "            sentence_lengths = batch[1]\n",
    "            targets = batch[2].to(device)\n",
    "            logits = model(sentences, sentence_lengths)\n",
    "            # logits => [batch_size, num_labels]\n",
    "\n",
    "            loss = criterion(logits, targets)\n",
    "            acc = categorical_accuracy(logits, targets)\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if i % 2000 == 1999:\n",
    "                wandb.log({\"val_loss\": epoch_loss/2000})\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea3127c2-ceef-4794-9a67-da46427c6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = elapsed_time - (elapsed_mins * 60)\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "22c99a8b-8813-4e32-a309-bb2c4245f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config\n",
    "config.learning_rate = lr\n",
    "config.learning_rate_decay = lr_decay\n",
    "config.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b156d202-69fd-4908-8306-08939b17611e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/Multilingual-Sentiment-Analysis/wandb/run-20220621_185226-nkivqj8o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alokpadhi/Sentiment%20Analysis-Hinglish/runs/nkivqj8o\" target=\"_blank\">northern-sky-17</a></strong> to <a href=\"https://wandb.ai/alokpadhi/Sentiment%20Analysis-Hinglish\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 3.61s\n",
      "\tTrain Loss: 1.086 | Val Loss: 1.060 | Val Acc: 0.407\n",
      "Epoch: 02 | Time: 0m 3.44s\n",
      "\tTrain Loss: 1.063 | Val Loss: 0.998 | Val Acc: 0.490\n",
      "Epoch: 03 | Time: 0m 3.47s\n",
      "\tTrain Loss: 1.016 | Val Loss: 0.968 | Val Acc: 0.490\n",
      "Epoch: 04 | Time: 0m 3.56s\n",
      "\tTrain Loss: 0.989 | Val Loss: 0.960 | Val Acc: 0.491\n",
      "Epoch: 05 | Time: 0m 3.59s\n",
      "\tTrain Loss: 0.975 | Val Loss: 0.949 | Val Acc: 0.502\n",
      "Epoch: 06 | Time: 0m 3.53s\n",
      "\tTrain Loss: 0.961 | Val Loss: 0.941 | Val Acc: 0.509\n",
      "Epoch: 07 | Time: 0m 3.35s\n",
      "\tTrain Loss: 0.950 | Val Loss: 0.935 | Val Acc: 0.518\n",
      "Epoch: 08 | Time: 0m 3.65s\n",
      "\tTrain Loss: 0.945 | Val Loss: 0.931 | Val Acc: 0.524\n",
      "Epoch: 09 | Time: 0m 3.39s\n",
      "\tTrain Loss: 0.934 | Val Loss: 0.938 | Val Acc: 0.524\n",
      "Epoch: 10 | Time: 0m 3.48s\n",
      "\tTrain Loss: 0.931 | Val Loss: 0.921 | Val Acc: 0.540\n",
      "Epoch: 11 | Time: 0m 3.56s\n",
      "\tTrain Loss: 0.925 | Val Loss: 0.923 | Val Acc: 0.538\n",
      "Epoch: 12 | Time: 0m 3.39s\n",
      "\tTrain Loss: 0.917 | Val Loss: 0.924 | Val Acc: 0.543\n",
      "Epoch: 13 | Time: 0m 3.52s\n",
      "\tTrain Loss: 0.911 | Val Loss: 0.919 | Val Acc: 0.547\n",
      "Epoch: 14 | Time: 0m 3.48s\n",
      "\tTrain Loss: 0.903 | Val Loss: 0.920 | Val Acc: 0.546\n",
      "Epoch: 15 | Time: 0m 3.32s\n",
      "\tTrain Loss: 0.897 | Val Loss: 0.918 | Val Acc: 0.556\n",
      "Epoch: 16 | Time: 0m 3.36s\n",
      "\tTrain Loss: 0.900 | Val Loss: 0.911 | Val Acc: 0.560\n",
      "Epoch: 17 | Time: 0m 3.43s\n",
      "\tTrain Loss: 0.894 | Val Loss: 0.911 | Val Acc: 0.564\n",
      "Epoch: 18 | Time: 0m 3.60s\n",
      "\tTrain Loss: 0.885 | Val Loss: 0.921 | Val Acc: 0.559\n",
      "Epoch: 19 | Time: 0m 4.01s\n",
      "\tTrain Loss: 0.881 | Val Loss: 0.916 | Val Acc: 0.566\n",
      "Epoch 00020: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch: 20 | Time: 0m 3.30s\n",
      "\tTrain Loss: 0.875 | Val Loss: 0.916 | Val Acc: 0.568\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='34.212 MB of 34.212 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">northern-sky-17</strong>: <a href=\"https://wandb.ai/alokpadhi/Sentiment%20Analysis-Hinglish/runs/nkivqj8o\" target=\"_blank\">https://wandb.ai/alokpadhi/Sentiment%20Analysis-Hinglish/runs/nkivqj8o</a><br/>Synced 5 W&B file(s), 0 media file(s), 13 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220621_185226-nkivqj8o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"Sentiment Analysis-Hinglish\", config=config) as run:\n",
    "    wandb.watch(model, criterion, log=\"all\")\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_loss = train(train_dataloader)\n",
    "        val_loss, val_acc = evaluate(val_dataloader)\n",
    "        end_time = time.time()\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        print(f\"Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s\")\n",
    "        print(f\"\\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.3f}\")\n",
    "\n",
    "        if val_loss < best_valid_loss:\n",
    "            best_valid_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        model_artifact = wandb.Artifact(\"GRUModel\", type=\"model\")\n",
    "        model_artifact.add_file(model_path)\n",
    "        run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1a2b6f6f-5aec-4680-bcf0-1f94a965e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            sentences = batch[0].to(device)\n",
    "            sentence_lengths = batch[1]\n",
    "            targets_ = batch[2].to(device)\n",
    "            predictions_ = model(sentences, sentence_lengths)\n",
    "            \n",
    "            outputs = predictions_.max(dim=1)[1]\n",
    "            targets.extend(targets_.detach().cpu().numpy().tolist())\n",
    "            predictions.extend(outputs.detach().cpu().numpy().tolist())\n",
    "            \n",
    "    return metrics.classification_report(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0e5a060c-585d-411a-8483-b243ad9f6c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.59      0.63       982\n",
      "           1       0.51      0.78      0.61       890\n",
      "           2       0.49      0.34      0.40      1128\n",
      "\n",
      "    accuracy                           0.55      3000\n",
      "   macro avg       0.56      0.57      0.55      3000\n",
      "weighted avg       0.56      0.55      0.54      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_metrics(model, test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281092c7-6e6b-435c-955e-0a1e0b9be25f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
